---
title: Backtesting
description: "Evaluate new system prompt on historical data"
---

Evaluate the performance of a new system prompt on historical data using the backtest endpoint.

This endpoint lets you call an LLM model on multiple messages in **parallel**, **evaluate** the results, and get a performance **report.**

Full API specs: [here](https://api.phospho.ai/v3/redoc#tag/Run/operation/post_run_backtests_run_backtest_post)

# Usage 

To run a backtest, you need to:

1. Log data to a phospho project
2. Setup analytics
3. Call the backtest endpoint
4. Discover the results

# Log data to a phospho project

To log data to a phospho project, you can use the [phospho API](/api-reference/introduction), import a file to the platform, or use one of the [phospho SDK](/http://localhost:3001/getting-started#log-to-phospho).

# Setup analytics in the project

In the phospho platform, go to the **Analytics** tab to setup the analytics you want to use in the backtest.

These analytics can be:
- Tagger: eg. topic of discussion
- Scorer: eg. sentiment
- Classifier: eg. user intent

Use the analytics to define how to evaluate the results. 

Learn more about the available [analytics in phospho here](/analytics/events).

# Call the backtest endpoint

The backtest endpoint does the following:

1. Fetch the phospho project data using `filters`
2. Create a system prompt from the `system_prompt_template` and `system_prompt_variables`
3. Call the LLM model specified in `provider_and_model` with the `OPENAI_API_KEY` set in the request' headers
4. Log back the results to the phospho project using the `version_id`

<Card href="https://api.phospho.ai/v3/redoc#tag/Run/operation/post_run_backtests_run_backtest_post" title="API specs" >
Read the full backtest API specs here
</Card>

## How does the template work?

The system prompt is created using a template and variables.

For example, the template could be:

```python
system_prompt_template = "The user is asking about {topic}. The user is {username}.""
system_prompt_variables = {
    "topic": "the weather",
    "username": "John"
}
```

The implementation is as follows:

```python
async def run_model(message: phospho.lab.Message) -> Optional[str]:
    """
    This is what the backtest endpoint does:
    """
    # Create a system prompt from the template and variables
    system_prompt = system_prompt_template.format(**system_prompt_variables)
    # Call the LLM model
    response = await client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": message.role, "content": message.content},
        ],
    )
    response_text = response.choices[0].message.content
```

## How to filter the data called during backtesting?

Use the optional `filters` parameter to select which data to use in the backtest.

```json
{
  "filters": {
    "created_at_start": 0, // timestamp in seconds
    "created_at_end": 0, // timestamp in seconds
    "event_name": ["string"],
    "event_id": ["string"],
    "flag": "string",
    "metadata": {},
    "user_id": "string",
    "last_eval_source": "string",
    "sentiment": "string",
    "language": "string",
    "has_notes": true,
    "tasks_ids": ["string"],
    "clustering_id": "string",
    "clusters_ids": ["string"],
    "is_last_task": true,
    "sessions_ids": ["string"]
  }
}
```

# Results

Results are available in the **AB tests** tab of the phospho platform.

This tab lets you compare the analytics on the original version, with the candidate version generated by the backtest.

<img height="200" src="/images/explore/abtest.jpeg" />

<Card href="/analytics/ab-test" title="AB tests" >
Learn more about AB tests in phospho
</Card>