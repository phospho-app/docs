---
title: Log to phospho with Python
description: "Collect interactions and tasks"
---

## Log tasks to phospho

**Tasks are the basic bricks that make up your LLM apps.** If you're a programmer, you can think of tasks like functions.

A task is made of at least two things:

- `input (str)`: What goes into a task. Eg: what the user asks to the assistant.
- `output (Optional[str])`: What goes out of the task. Eg: what the assistant replied to the user.

Example of tasks you can log to phospho:

- Call to an LLM (input = query, output = llm response)
- Answering a question (input = question, output = answer)
- Searching in documents (input = search query, output = document)
- Summarizing a text (input = text, output = summary)
- Performing inference of a model (input = X, output = y)

## Install phospho module

The phospho [Python module](https://pypi.org/project/phospho/) in the easiest way to log to phospho. It is compatible with Python 3.9+.

```bash
pip install --upgrade phospho
```

<Info>
  The phospho module is an open source work in progress. [Your help is deeply
  appreciated!](https://github.com/phospho-app/phospho)
</Info>

## Initialize phospho

In your app, initialize the phospho module. By default, phospho will look for `PHOSPHO_API_KEY` and `PHOSPHO_PROJECT_ID` environment variables.

<Tip>
  Learn how to get your api key and project id by [clicking
  here!](getting-started##1-get-your-phospho-api-key-and-your-project-id)
</Tip>

```python
import phospho

phospho.init()
```

You can also pass the `api_key` and `project_id` parameters to `phospho.init`.

```python
phospho.init(api_key="phospho-key", project_id="phospho-project-id")
```

## Log with phospho.log

The most minimal way to log a task is to use `phospho.log`.

### Log text input and output

phospho is a text analytics tool. You can log any string input and output this way:

```python
input_text = "Hello! This is what the user asked to the system"
output_text = "This is the response showed to the user by the app."

# This is how you log a task to phospho
phospho.log(input=input_text, output=output_text)
```

The output is optional.

The input and output logged to phospho are used to perform text analytics. For example, sentiment analysis, intent detection, or evaluation.

### Log OpenAI queries and responses

phospho aims to be battery included. So if you pass something else than a `str` to `phospho.log`, phospho extracts what's usually considered "the input" or "the output".

For example, you can pass to `phospho.log` the same `input` as the arguments for `openai.chat.completions.create`. And you can pass to `phospho.log` the same `output` as OpenAI's `ChatCompletion` objects.

```python
import openai
import phospho

phospho.init()
openai_client = openai.OpenAI(api_key="openai-key")

input_prompt = "Explain quantum computers in less than 20 words."

# This is your LLM app code
query = {
    "messages": [{"role": "system", "content": "You are a helpful assistant."},
                 {"role": "user", "content": input_prompt},
    ],
    "model": "gpt-3.5-turbo",
}
response = openai_client.chat.completions.create(**query)

# You can directly pass as dict or a ChatCompletion as input and output
log = phospho.log(input=query, output=response)
print("input:", log["input"])
print("output:", log["output"])
```

Result:

```text
input: Explain quantum computers in less than 20 words.
output: Qubits harness quantum physics for faster, more powerful computation.
```

### Log a list of OpenAI messages 

In conversational apps, your conversation history is often a list of messages. You can directly log this list as an input or an output. 

The input, output, and system prompt are automatically extracted based on the messages' role.

```python

# This is your conversation history
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Explain quantum computers in less than 20 words."},
]

# Your LLM app code generates a response
response = openai_client.chat.completions.create(
    messages=messages,
    model="gpt-3.5-turbo",
)

# You append the response to the conversation history
messages.append({"role": response.choices[0].role, "content": response.choices[0].message.content, } )

# You can log the conversation history as input or output
log = phospho.log(input=messages, output=messages)

print("input:", log["input"])
print("output:", log["output"])
print("system_prompt:", log["system_prompt"]) # system prompt is automatically extracted
```

Result:

```text
input: Explain quantum computers in less than 20 words.
output: Qubits harness quantum physics for faster, more powerful computation.
system_prompt: You are a helpful assistant.
```

Note that consecutive messages with the same role are **concatenated** with a newline.

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Explain quantum computers in less than 20 words."},
    {"role": "user", "content": "What is the speed of light?"},
]
log = phospho.log(input=messages)
```

Result:

```text
input: Explain quantum computers in less than 20 words.\nWhat is the speed of light?
```

If you need more control, consider using custom extractors.

### Custom extractors

Pass custom extractors to `phospho.log` to extract the input and output from any object. The custom extractor is a function that is applied to the input or output before logging. The function should return a string.

The original object is converted to a dict (if jsonable) or a string, and stored in `raw_input` and `raw_output`.

```python
phospho.log(
    input={"custom_input": "this is a complex object"},
    output={"custom_output": "which is not a string nor a standard object"},
    # Custom extractors return a string
    input_to_str_function=lambda x: x["custom_input"],
    output_to_str_fucntion=lambda x: x["custom_output"],
)
```

Result:

```text
input: this is a complex object
output: which is not a string nor a standard object
```

## Logging additional metadata

You can log additional data with each interaction (user id, version id,...) by passing arguments to `phospho.log`.

```python
log = phospho.log(
    input="log this",
    output="and that",
    # There is a metadata field
    metadata={"always": "moooore"},
    # Every extra keyword argument is logged as metadata
    log_anything_and_everything="even this is ok",
)
```

## Wrapping functions' arguments and values

Wrap any function with `phospho.wrap` to automatically log an interaction when they are called.

- The passed arguments are logged as input
- The returned value is logged as output

### Use the phospho.wrap decorator

```python
@phospho.wrap
def answer(messages: List[Dict[str, str]]) -> Optional[str]:
    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
    )
    return response.choices[0].delta.content
```

Like phospho.log, every extra keyword argument is logged as metadata.

### How to log metadata with phospho.wrap?

```python
@phospho.wrap(metadata={"more": "details"})
def answer(messages: List[Dict[str, str]]) -> Optional[str]:
    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
    )
    return response.choices[0].delta.content
```

### Wrap an imported function with phospho.wrap

If you can't change the function definition, you can wrap it this way.

```python
# You can wrap any function call in phospho.wrap
response = phospho.wrap(
    openai_client.chat.completions.create,
    # Pass additional metadata
    metadata={"more": "details"},
)(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain quantum computers in less than 20 words."},
    ],
    model="gpt-3.5-turbo",
)
```

If you want to wrap all calls to a function, you can override the function definition.

```python
openai_client.chat.completions.create = phospho.wrap(
    openai_client.chat.completions.create
)
```

## Streaming

phospho supports streamed outputs. This is useful when you want to log the output of a streaming API.

### Example with phospho.log

Pass `stream=True` to `phospho.log` to handle streaming responses. When iterating over the response, phospho will automatically log each chunk until the iteration is completed.

For example, you can pass streaming OpenAI responses to `phospho.log` the following way:

```python

from openai.types.chat import ChatCompletionChunk
from openai._streaming import Stream

query = {
    "messages": [{"role": "system", "content": "You are a helpful assistant."},
                 {"role": "user", "content": "Explain quantum computers in less than 20 words."},
    ],
    "model": "gpt-3.5-turbo",
    # Enable streaming on OpenAI
    "stream": True
}
# OpenAI completion function return a Stream of chunks
response: Stream[ChatCompletionChunk] = openai_client.chat.completions.create(**query)

# Pass stream=True to phospho.log to handle this
phospho.log(input=query, output=response, stream=True)
```

### Example with phospho.wrap

- Pass `stream=True`. This tells phospho to concatenate the string outputs.
- Pass a `stop` function, such that `stop(output) is True` when the streaming is finished and trigger the logging of the task.

```python
@phospho.wrap(stream=True, stop=lambda token: token is None)
def answer(messages: List[Dict[str, str]]) -> Generator[Optional[str], Any, None]:
    streaming_response: Stream[
        ChatCompletionChunk
    ] = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        stream=True,
    )
    for response in streaming_response:
        yield response.choices[0].delta.content
```

### Example with a generator

For generators, you need to wrap it with `phospho.MutableGenerator` or `phospho.MutableAsyncGenerator`.

```python
# Example with an Ollama endpoint

r = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": ...,
        "prompt": ...,
        "context": ...,
    },
    # This connects to a streaming API endpoint
    stream=True,
)
r.raise_for_status()
response_iterator = r.iter_lines()

# In order to directly log this to phospho, we need to wrap it this way
response_iterator = phospho.MutableGenerator(
    generator=response_iterator,
    # Indicate when the streaming stops
    stop=lambda line: json.loads(line).get("done", False),
)

# Log the generated content to phospho with Stream=True
phospho.log(input=prompt, output=response_iterator, stream=True)

# As you iterate over the response, phospho combines the chunks
# When stop(output) is True, the iteration is completed and the task is logged
for line in response_iterator:
    print(line)
```
